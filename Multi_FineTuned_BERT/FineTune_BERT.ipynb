{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjM0UimlfeDc"
      },
      "source": [
        "# Multi Lingual BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3YZ17cgfcdj",
        "outputId": "4f55f88f-fc1d-4764-a29f-166123728a3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(34800, 2)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Ensure the directory exists, create it if it doesn't\n",
        "save_directory = r'E:\\Bangla-Sentiment-Analysis\\Multi_FineTuned_BERT\\New_Multi_FineTuned_4'\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_excel(r\"E:\\Bangla-Sentiment-Analysis\\Bangla_Dataset\\final_preprocessed_dataset.xlsx\")\n",
        "\n",
        "#df = df[['clean_sentence', 'Sentiment']]\n",
        "#df2 = pd.read_excel(r\"C:\\Users\\Rizvi\\Desktop\\Bilstm_Bangla\\product_reviews_bn_translated.xlsx\")\n",
        "#df2.head()\n",
        "#df2 = df2[['translated_sentence', 'Sentiment']]\n",
        "# Rename columns to have a common name for reviews\n",
        "#df2 = df2.rename(columns={'translated_sentence': 'clean_sentence'})\n",
        "#df = df.rename(columns={'clean_sentence': 'clean_sentence'})\n",
        "\n",
        "# Concatenate the DataFrames vertically\n",
        "#all_reviews_df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
        "\n",
        "# Print the shape of the concatenated DataFrame\n",
        "#print(\"Shape of the concatenated DataFrame:\", all_reviews_df.shape)\n",
        "#df=all_reviews_df\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "32JEQ-bhkBql",
        "outputId": "5bcf336d-6c41-4776-cfc6-d84aba25654d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Rizvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\Rizvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "C:\\Users\\Rizvi\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10440' max='10440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10440/10440 1:40:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.070900</td>\n",
              "      <td>1.052187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.023100</td>\n",
              "      <td>1.027815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.970900</td>\n",
              "      <td>1.004685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.916000</td>\n",
              "      <td>0.890642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.892400</td>\n",
              "      <td>0.841093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.887400</td>\n",
              "      <td>0.818589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.873900</td>\n",
              "      <td>0.874492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.878900</td>\n",
              "      <td>0.831631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.916700</td>\n",
              "      <td>0.964740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.840300</td>\n",
              "      <td>0.841511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.855400</td>\n",
              "      <td>0.864574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.839600</td>\n",
              "      <td>0.852988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.886037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.936100</td>\n",
              "      <td>0.832286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.852400</td>\n",
              "      <td>0.829012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.896200</td>\n",
              "      <td>0.865646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.891800</td>\n",
              "      <td>0.862205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.855800</td>\n",
              "      <td>0.887506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.842000</td>\n",
              "      <td>0.991329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.839853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.889800</td>\n",
              "      <td>0.817364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.828800</td>\n",
              "      <td>0.836029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.865200</td>\n",
              "      <td>0.851628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.845400</td>\n",
              "      <td>0.914723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.839900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.844100</td>\n",
              "      <td>0.817068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.854100</td>\n",
              "      <td>0.851378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.834200</td>\n",
              "      <td>0.863981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.779900</td>\n",
              "      <td>0.849934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.816800</td>\n",
              "      <td>0.882150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.793300</td>\n",
              "      <td>0.865968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.840100</td>\n",
              "      <td>0.849333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.853800</td>\n",
              "      <td>0.814942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.830100</td>\n",
              "      <td>0.821555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.852800</td>\n",
              "      <td>0.990613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.797000</td>\n",
              "      <td>0.888006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.816600</td>\n",
              "      <td>0.816748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.811500</td>\n",
              "      <td>0.820387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.837300</td>\n",
              "      <td>0.851102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.935966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.843000</td>\n",
              "      <td>0.899953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.856600</td>\n",
              "      <td>0.835459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.828000</td>\n",
              "      <td>0.826652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.859100</td>\n",
              "      <td>0.817959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.795000</td>\n",
              "      <td>0.831147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.803000</td>\n",
              "      <td>0.858596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.802800</td>\n",
              "      <td>0.890591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.814700</td>\n",
              "      <td>0.812001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.788100</td>\n",
              "      <td>0.857054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.827100</td>\n",
              "      <td>0.813322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.792000</td>\n",
              "      <td>0.802611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.781200</td>\n",
              "      <td>0.807100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.776600</td>\n",
              "      <td>0.840833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.824600</td>\n",
              "      <td>0.807797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.812700</td>\n",
              "      <td>0.824907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.856500</td>\n",
              "      <td>0.796340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>0.771000</td>\n",
              "      <td>0.851970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.761900</td>\n",
              "      <td>0.796812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>0.784800</td>\n",
              "      <td>0.823747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.800100</td>\n",
              "      <td>0.819788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>0.833600</td>\n",
              "      <td>0.792632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>0.782800</td>\n",
              "      <td>0.804410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>0.789700</td>\n",
              "      <td>0.788825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.765000</td>\n",
              "      <td>0.821736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.770100</td>\n",
              "      <td>0.791914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>0.804800</td>\n",
              "      <td>0.791898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>0.773600</td>\n",
              "      <td>0.790362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.728900</td>\n",
              "      <td>0.806509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>0.789600</td>\n",
              "      <td>0.786340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.705700</td>\n",
              "      <td>0.845502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>0.796600</td>\n",
              "      <td>0.811783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.713200</td>\n",
              "      <td>0.809814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>0.767000</td>\n",
              "      <td>0.786576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>0.792100</td>\n",
              "      <td>0.779879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.782700</td>\n",
              "      <td>0.787227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.762000</td>\n",
              "      <td>0.795691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>0.802300</td>\n",
              "      <td>0.781403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>0.781300</td>\n",
              "      <td>0.782478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>0.743000</td>\n",
              "      <td>0.783411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.777300</td>\n",
              "      <td>0.787368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>0.743500</td>\n",
              "      <td>0.784480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>0.722000</td>\n",
              "      <td>0.793056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>0.693500</td>\n",
              "      <td>0.797276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.736600</td>\n",
              "      <td>0.809853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.728500</td>\n",
              "      <td>0.788672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>0.735900</td>\n",
              "      <td>0.766072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>0.728300</td>\n",
              "      <td>0.774591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.722700</td>\n",
              "      <td>0.778778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>0.739900</td>\n",
              "      <td>0.777135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.741000</td>\n",
              "      <td>0.774401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>0.685200</td>\n",
              "      <td>0.773620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>0.705500</td>\n",
              "      <td>0.773935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>0.727900</td>\n",
              "      <td>0.778902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>0.745900</td>\n",
              "      <td>0.769520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>0.774805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.727800</td>\n",
              "      <td>0.782874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>0.757200</td>\n",
              "      <td>0.774017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>0.722900</td>\n",
              "      <td>0.766686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>0.741800</td>\n",
              "      <td>0.760196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.676900</td>\n",
              "      <td>0.766698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>0.705100</td>\n",
              "      <td>0.768383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>0.674800</td>\n",
              "      <td>0.771880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>0.745600</td>\n",
              "      <td>0.765907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>0.729200</td>\n",
              "      <td>0.764825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('C:\\\\Users\\\\Rizvi\\\\Desktop\\\\New_Multi_FineTuned_4\\\\tokenizer_config.json',\n",
              " 'C:\\\\Users\\\\Rizvi\\\\Desktop\\\\New_Multi_FineTuned_4\\\\special_tokens_map.json',\n",
              " 'C:\\\\Users\\\\Rizvi\\\\Desktop\\\\New_Multi_FineTuned_4\\\\vocab.txt',\n",
              " 'C:\\\\Users\\\\Rizvi\\\\Desktop\\\\New_Multi_FineTuned_4\\\\added_tokens.json')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts = df['clean_sentence'].tolist()  # Adjust this to the actual text column\n",
        "labels = df['Sentiment'].tolist()  # Adjust this to the actual label column\n",
        "\n",
        "# Map string labels to numeric values\n",
        "label_map = {'Positive': 0, 'Negative': 1, 'Neutral': 2}  # Adjust the mapping if necessary\n",
        "numeric_labels = [label_map[label] for label in labels]\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, numeric_labels, test_size=0.2)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize the input texts\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure labels are long type for classification\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "\n",
        "# Load the BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(set(numeric_labels)))\n",
        "\n",
        "# Check if a GPU is available and set device accordingly\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    #learning_rate=0.01,              # Learning rate\n",
        "    output_dir=save_directory,  # Directory to store the results\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=1740,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir=os.path.join(save_directory, 'logs'),  # Log directory\n",
        "    evaluation_strategy=\"steps\",  # Evaluate every few steps\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    save_steps=500,  # Save the model every 500 steps\n",
        "    save_total_limit=2, # Only keep the 2 latest models\n",
        "    eval_steps=100,  # Evaluate every 100 steps\n",
        "    load_best_model_at_end=True,  # Load the best model when done\n",
        "    metric_for_best_model=\"eval_loss\",  # Early stop based on validation loss\n",
        ")\n",
        "\n",
        "# Create Trainer instance with EarlyStoppingCallback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=6)],  # Stop if no improvement after 3 evaluations\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltRRmxBop299"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
